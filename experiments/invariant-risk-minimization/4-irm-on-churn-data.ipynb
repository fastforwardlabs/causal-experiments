{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRM on churn data\n",
    "\n",
    "The news popularity dataset seems hard to model. Let's try our old favourite churn dataset.\n",
    "\n",
    "For IRM to work, we need to be able to define several _environments_ on which to train. An environment is the result of an intervention - something that changed the data generating process. The environments need to be sufficiently different (and sufficiently similar). Then IRM will return to us an invariant representation - one that has learned the correlations that hold true across environments, but ignored spurious correlations specific to an environment.\n",
    "\n",
    "For the churn dataset, it's not clear what an environment could be. Let's construct a plausible business story: our telco company has lots of data for single people with no dependents, (it has previously marketed to that demographic), but is launching a family oriented brand. It needs to do churn modeling on the family brand, but has little data. As such, we'll construct four environments from the features \"Partner\" and \"Dependents\", reserving the case where both Partner and Dependents are true as the test environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/churn.csv').drop(['customerID','TotalCharges'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a machine learnable dataset. It'll give us some weird column names, but this is exploration, we'll deal with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since during IRM we'll be training on several environments, we wrap each in a dict for easy management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_env(df):\n",
    "    return {\n",
    "        'features': torch.Tensor(\n",
    "            df.drop(['Churn_Yes', 'Partner_Yes', 'Dependents_Yes'],\n",
    "                    axis='columns').to_numpy()),\n",
    "        'target': torch.Tensor(df['Churn_Yes'].to_numpy()).unsqueeze(dim=1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our neural net architecture. We're starting with a straightforward MLP with ReLU nonlinearities and a sigmoid output, since it's a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim):\n",
    "        super(NN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_features, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = torch.sigmoid(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a bunch of utility functions for calculating errors and such to report during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(logits, target):\n",
    "    loss = nn.functional.binary_cross_entropy(logits, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty(logits, target):\n",
    "    dummy = torch.tensor(1., requires_grad=True)\n",
    "    loss = error(logits*dummy, target)\n",
    "    grad = autograd.grad(loss, [dummy], create_graph=True)[0]\n",
    "    squared_grad_norm = (grad**2).sum()\n",
    "    return squared_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, target):\n",
    "    n_preds = torch.tensor(len(predictions)).float()\n",
    "    acc = ((predictions == target).sum() / n_preds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(predictions, target):\n",
    "    n_preds = torch.tensor(len(predictions)).float()\n",
    "    tp = ((predictions == 1) & (target == 1)).sum().float()\n",
    "    fp = ((predictions == 1) & (target == 0)).sum().float()\n",
    "    prec = tp / (tp + fp)\n",
    "    return prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(predictions, target):\n",
    "    n_preds = torch.tensor(len(predictions)).float()\n",
    "    tp = ((predictions == 1) & (target == 1)).sum().float()\n",
    "    fn = ((predictions == 0) & (target == 1)).sum().float()\n",
    "    rec = tp / (tp + fn)\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct environments. We hold out a final test set (the naming of validation and test is arguably the wrong way around here) of customers with dependents and partners. We train on two environments, both are customers without partners, and the two envs are defined by whether they have dependents or not. The validation set is the remaining combination (with partner, without dependents). We can use this to choose an early stopping time.\n",
    "\n",
    "To start a new training procedure, we need to run all the code below here, since the environments are mutable dictionaries that pick up entries during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = construct_env(df_[(df_.Partner_Yes == 1) & (df_.Dependents_Yes == 1)])\n",
    "env_valid = construct_env(df_[(df_.Partner_Yes == 1) & (df_.Dependents_Yes == 0)])\n",
    "env_1 = construct_env(df_[(df_.Partner_Yes == 0) & (df_.Dependents_Yes == 1)])\n",
    "env_2 = construct_env(df_[(df_.Partner_Yes == 0) & (df_.Dependents_Yes == 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = env_1['features'].shape[1]\n",
    "HIDDEN_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NN(N_FEATURES, HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "iteration: 0, train_loss: 0.64038801\n",
      "env_1 accuracy: 0.729, precision: 0.182, and recall: 0.078\n",
      "env_2 accuracy: 0.628, precision: 0.303, and recall: 0.066\n",
      "validation accuracy: 0.742, precision: 0.364, and recall: 0.019\n",
      "---\n",
      "iteration: 1000, train_loss: 0.42862311\n",
      "env_1 accuracy: 0.834, precision: 0.660, and recall: 0.455\n",
      "env_2 accuracy: 0.753, precision: 0.655, and recall: 0.589\n",
      "validation accuracy: 0.806, precision: 0.653, and recall: 0.502\n",
      "---\n",
      "iteration: 2000, train_loss: 0.41983685\n",
      "env_1 accuracy: 0.848, precision: 0.704, and recall: 0.494\n",
      "env_2 accuracy: 0.757, precision: 0.657, and recall: 0.608\n",
      "validation accuracy: 0.804, precision: 0.651, and recall: 0.493\n",
      "---\n",
      "iteration: 3000, train_loss: 0.41280931\n",
      "env_1 accuracy: 0.859, precision: 0.717, and recall: 0.558\n",
      "env_2 accuracy: 0.755, precision: 0.654, and recall: 0.602\n",
      "validation accuracy: 0.805, precision: 0.654, and recall: 0.490\n",
      "---\n",
      "iteration: 4000, train_loss: 0.40702993\n",
      "env_1 accuracy: 0.861, precision: 0.729, and recall: 0.558\n",
      "env_2 accuracy: 0.757, precision: 0.651, and recall: 0.624\n",
      "validation accuracy: 0.800, precision: 0.634, and recall: 0.502\n",
      "---\n",
      "iteration: 5000, train_loss: 0.40493298\n",
      "env_1 accuracy: 0.856, precision: 0.719, and recall: 0.532\n",
      "env_2 accuracy: 0.755, precision: 0.649, and recall: 0.615\n",
      "validation accuracy: 0.799, precision: 0.637, and recall: 0.486\n",
      "---\n",
      "iteration: 6000, train_loss: 0.40419406\n",
      "env_1 accuracy: 0.870, precision: 0.742, and recall: 0.597\n",
      "env_2 accuracy: 0.752, precision: 0.644, and recall: 0.619\n",
      "validation accuracy: 0.797, precision: 0.629, and recall: 0.488\n",
      "---\n",
      "iteration: 7000, train_loss: 0.40371937\n",
      "env_1 accuracy: 0.867, precision: 0.738, and recall: 0.584\n",
      "env_2 accuracy: 0.754, precision: 0.647, and recall: 0.622\n",
      "validation accuracy: 0.798, precision: 0.630, and recall: 0.495\n",
      "---\n",
      "iteration: 8000, train_loss: 0.40355647\n",
      "env_1 accuracy: 0.867, precision: 0.746, and recall: 0.571\n",
      "env_2 accuracy: 0.755, precision: 0.652, and recall: 0.610\n",
      "validation accuracy: 0.797, precision: 0.634, and recall: 0.474\n",
      "---\n",
      "iteration: 9000, train_loss: 0.40302932\n",
      "env_1 accuracy: 0.873, precision: 0.738, and recall: 0.623\n",
      "env_2 accuracy: 0.751, precision: 0.640, and recall: 0.624\n",
      "validation accuracy: 0.797, precision: 0.625, and recall: 0.507\n",
      "---\n",
      "iteration: 10000, train_loss: 0.40256894\n",
      "env_1 accuracy: 0.870, precision: 0.750, and recall: 0.584\n",
      "env_2 accuracy: 0.753, precision: 0.647, and recall: 0.616\n",
      "validation accuracy: 0.796, precision: 0.628, and recall: 0.483\n",
      "---\n",
      "iteration: 11000, train_loss: 0.40233755\n",
      "env_1 accuracy: 0.870, precision: 0.759, and recall: 0.571\n",
      "env_2 accuracy: 0.754, precision: 0.650, and recall: 0.607\n",
      "validation accuracy: 0.798, precision: 0.637, and recall: 0.476\n",
      "---\n",
      "iteration: 12000, train_loss: 0.40195841\n",
      "env_1 accuracy: 0.870, precision: 0.759, and recall: 0.571\n",
      "env_2 accuracy: 0.754, precision: 0.650, and recall: 0.609\n",
      "validation accuracy: 0.797, precision: 0.633, and recall: 0.476\n",
      "---\n",
      "iteration: 13000, train_loss: 0.40185291\n",
      "env_1 accuracy: 0.864, precision: 0.726, and recall: 0.584\n",
      "env_2 accuracy: 0.753, precision: 0.644, and recall: 0.622\n",
      "validation accuracy: 0.797, precision: 0.629, and recall: 0.493\n",
      "---\n",
      "iteration: 14000, train_loss: 0.40176278\n",
      "env_1 accuracy: 0.870, precision: 0.759, and recall: 0.571\n",
      "env_2 accuracy: 0.754, precision: 0.651, and recall: 0.607\n",
      "validation accuracy: 0.797, precision: 0.634, and recall: 0.474\n",
      "---\n",
      "iteration: 15000, train_loss: 0.40163738\n",
      "env_1 accuracy: 0.870, precision: 0.759, and recall: 0.571\n",
      "env_2 accuracy: 0.753, precision: 0.650, and recall: 0.607\n",
      "validation accuracy: 0.797, precision: 0.632, and recall: 0.479\n",
      "---\n",
      "iteration: 16000, train_loss: 0.40153930\n",
      "env_1 accuracy: 0.870, precision: 0.734, and recall: 0.610\n",
      "env_2 accuracy: 0.753, precision: 0.644, and recall: 0.627\n",
      "validation accuracy: 0.797, precision: 0.624, and recall: 0.502\n",
      "---\n",
      "iteration: 17000, train_loss: 0.40143937\n",
      "env_1 accuracy: 0.861, precision: 0.721, and recall: 0.571\n",
      "env_2 accuracy: 0.754, precision: 0.647, and recall: 0.618\n",
      "validation accuracy: 0.796, precision: 0.629, and recall: 0.481\n",
      "---\n",
      "iteration: 18000, train_loss: 0.40142900\n",
      "env_1 accuracy: 0.870, precision: 0.759, and recall: 0.571\n",
      "env_2 accuracy: 0.755, precision: 0.653, and recall: 0.606\n",
      "validation accuracy: 0.798, precision: 0.637, and recall: 0.476\n",
      "---\n",
      "iteration: 19000, train_loss: 0.40103304\n",
      "env_1 accuracy: 0.867, precision: 0.746, and recall: 0.571\n",
      "env_2 accuracy: 0.754, precision: 0.650, and recall: 0.612\n",
      "validation accuracy: 0.797, precision: 0.634, and recall: 0.479\n",
      "---\n",
      "iteration: 20000, train_loss: 0.40093705\n",
      "env_1 accuracy: 0.873, precision: 0.738, and recall: 0.623\n",
      "env_2 accuracy: 0.752, precision: 0.642, and recall: 0.625\n",
      "validation accuracy: 0.796, precision: 0.623, and recall: 0.500\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(20001):\n",
    "    for env in [env_1, env_2]:\n",
    "        logits = net(env['features'])\n",
    "        env['error'] = error(logits, env['target'])\n",
    "        env['penalty'] = penalty(logits, env['target'])\n",
    "    \n",
    "    train_error = torch.stack([env_1['error'], env_2['error']]).mean()\n",
    "    train_penalty = torch.stack([env_1['penalty'], env_2['penalty']]).mean()\n",
    "    \n",
    "    # deactivate IRM to begin\n",
    "    total_loss = train_error #(train_error + 1e6 * train_penalty) / 1e6\n",
    "        \n",
    "    opt.zero_grad()\n",
    "    total_loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    valid_preds = net(env_valid['features']) > 0.5\n",
    "    env_1_preds = net(env_1['features']) > 0.5\n",
    "    env_2_preds = net(env_2['features']) > 0.5\n",
    "    test_preds = net(env_test['features']) > 0.5\n",
    "    \n",
    "    # ## train environment metrics\n",
    "    env_1['accuracy'] = accuracy(env_1_preds, env_1['target'])\n",
    "    env_1['precision'] = precision(env_1_preds, env_1['target'])\n",
    "    env_1['recall'] = recall(env_1_preds, env_1['target'])\n",
    "    \n",
    "    env_2['accuracy'] = accuracy(env_2_preds, env_2['target'])\n",
    "    env_2['precision'] = precision(env_2_preds, env_2['target'])\n",
    "    env_2['recall'] = recall(env_2_preds, env_2['target'])\n",
    "    \n",
    "    # ## validation set metrics\n",
    "    env_valid['accuracy'] = accuracy(valid_preds, env_valid['target'])\n",
    "    env_valid['precision'] = precision(valid_preds, env_valid['target'])\n",
    "    env_valid['recall'] = recall(valid_preds, env_valid['target'])\n",
    "    \n",
    "    if iteration % 1000 == 0:\n",
    "        print('---')\n",
    "        print('iteration: {}, train_loss: {:.8f}'.format(iteration, total_loss))\n",
    "        print('env_1 accuracy: {:.3f}, precision: {:.3f}, and recall: {:.3f}'''.format(\n",
    "            env_1['accuracy'], env_1['precision'], env_1['recall']\n",
    "        ))\n",
    "        print('env_2 accuracy: {:.3f}, precision: {:.3f}, and recall: {:.3f}'''.format(\n",
    "            env_2['accuracy'], env_2['precision'], env_2['recall']\n",
    "        ))\n",
    "        print('validation accuracy: {:.3f}, precision: {:.3f}, and recall: {:.3f}'''.format(\n",
    "            env_valid['accuracy'], env_valid['precision'], env_valid['recall']\n",
    "        ))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I ran the above both with and without IRM. The results are not very convincing: performance on the holdout environment practically doesn't change with or without. We'd like to see that the performance in the validation set is trash without IRM. I guess in this case, our environments are not sufficiently different, and as such even empirical risk minimization (ERM) returns a representation that is invariant across them.\n",
    "\n",
    "# with IRM\n",
    "\n",
    "iteration: 19000, train_loss: 0.000\n",
    "env_1 accuracy: 0.828, precision: 0.653, and recall: 0.416\n",
    "env_2 accuracy: 0.749, precision: 0.644, and recall: 0.594\n",
    "validation accuracy: 0.780, precision: 0.573, and recall: 0.533\n",
    "\n",
    "# without\n",
    "\n",
    "iteration: 19000, train_loss: 0.378\n",
    "env_1 accuracy: 0.861, precision: 0.721, and recall: 0.571\n",
    "env_2 accuracy: 0.757, precision: 0.661, and recall: 0.598\n",
    "validation accuracy: 0.783, precision: 0.591, and recall: 0.474\n",
    "\n",
    "\n",
    "this is not convincing - is there an effect at all? - but then, we have not done any hyperparameter searching, nor even defined a stopping criterion.\n",
    "\n",
    "also, perhaps the environments are too similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7459)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline accuracy (majority class predictor)\n",
    "(1-env_valid['target']).sum() / len(env_valid['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
